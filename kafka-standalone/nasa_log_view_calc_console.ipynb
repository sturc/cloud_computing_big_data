{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa App for nasa log view calculation (console output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the necessary variables\n",
    "# The following links were used to determine the necessary packages to include:\n",
    "# - https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html and \n",
    "# - https://github.com/OneCricketeer/docker-stacks/blob/master/hadoop-spark/spark-notebooks/kafka-sql.ipynb  \n",
    "\n",
    "scala_version = '2.12'  \n",
    "spark_version = '3.5.3'\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "topic_name = 'kappa-topic'\n",
    "window_duration = '1 minute'\n",
    "sliding_duration = '1 minute'\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.9.0'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "   .master(\"local\")\\\n",
    "   .appName(\"kafka-example\")\\\n",
    "   .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    "   .config(\"SQLConf.ADAPTIVE_EXECUTION_ENABLED.key\", \"false\")\\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaDf = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", *bootstrap_servers)\\\n",
    "  .option(\"subscribe\", topic_name)\\\n",
    "  .option(\"startingOffsets\", \"earliest\")\\\n",
    "  .load()\n",
    "print(kafkaDf.isStreaming)    # Returns True for DataFrames that have streaming sources\n",
    "kafkaDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure the value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit, split, from_unixtime\n",
    "#col_schema = [\"line_no INTEGER\",\"host STRING\",\"time TIMESTAMP\",\"method STRING\",\"url STRING\",\"response INTEGER\",\"bytes INTEGER\"]\n",
    "#schema_str = \",\".join(col_schema)\n",
    "structured_df = kafkaDf.select(\n",
    "    concat(col(\"topic\"), lit(':'), col(\"partition\").cast(\"string\")).alias(\"topic_partition\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"value\").cast(\"string\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"timestampType\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(0).alias(\"line_no\").cast(\"integer\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(1).alias(\"host\").cast(\"string\"),\n",
    "    from_unixtime(split(col(\"value\").cast(\"string\"),\",\").getItem(2)).alias(\"time\").cast(\"timestamp\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(3).alias(\"method\").cast(\"string\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(4).alias(\"url\").cast(\"string\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(5).alias(\"response\").cast(\"integer\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(6).alias(\"bytes\").cast(\"integer\"),   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window,endswith\n",
    "\n",
    "page_count_df = structured_df.filter(structured_df[\"method\"] == \"GET\")\\\n",
    "     .filter(structured_df[\"url\"].endswith(\".html\"))\\\n",
    "     .groupBy (window(\n",
    "        col(\"time\"),\n",
    "        window_duration,\n",
    "        sliding_duration\n",
    "    ),\n",
    "    col(\"url\")).count() \\\n",
    "     .withColumnRenamed('window.start', 'window_start') \\\n",
    "     .withColumnRenamed('window.end', 'window_end')\n",
    "\n",
    "query = page_count_df.writeStream\\\n",
    "     .format(\"console\")\\\n",
    "     .outputMode(\"update\") \\\n",
    "     .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
