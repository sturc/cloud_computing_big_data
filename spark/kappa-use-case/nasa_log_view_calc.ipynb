{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa App for nasa log view calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the necessary variables\n",
    "# The following links were used to determine the necessary packages to include:\n",
    "# - https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html and \n",
    "# - https://github.com/OneCricketeer/docker-stacks/blob/master/hadoop-spark/spark-notebooks/kafka-sql.ipynb  \n",
    "\n",
    "scala_version = '2.12'  \n",
    "spark_version = '3.5.3'\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "topic_name = 'kappa-topic'\n",
    "consumer_group_id = 'my_group_id'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.9.0'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "   .master(\"local\")\\\n",
    "   .appName(\"kafka-example\")\\\n",
    "   .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    "   .config(\"SQLConf.ADAPTIVE_EXECUTION_ENABLED.key\", \"false\")\\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 17:50:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/02/23 17:50:20 WARN StreamingQueryManager: Stopping existing streaming query [id=07d4ecd4-f3c5-4a24-87d0-59ce6eb47af9, runId=a84cc6d3-9791-40d0-b215-e689d7d9473a], as a new run is being started.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "kafkaDf = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", *bootstrap_servers)\\\n",
    "  .option(\"subscribe\", topic_name)\\\n",
    "  .option(\"startingOffsets\", \"earliest\")\\\n",
    "  .load()\n",
    "print(kafkaDf.isStreaming)    # Returns True for DataFrames that have streaming sources\n",
    "kafkaDf.printSchema()\n",
    "query = kafkaDf.select(\n",
    "    concat(col(\"topic\"), lit(':'), col(\"partition\").cast(\"string\")).alias(\"topic_partition\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"value\").cast(\"string\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"timestampType\")\n",
    "    ).writeStream\\\n",
    "      .format(\"console\")\\\n",
    "      .option(\"checkpointLocation\", \"/tmp\") \\\n",
    "      .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure the value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StreamingQuery' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m col_schema \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline_no INTEGER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime TIMESTAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse INTEGER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes INTEGER\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m schema_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(col_schema)\n\u001b[0;32m----> 6\u001b[0m df_csv \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m(from_csv(query\u001b[38;5;241m.\u001b[39mvalue, schema_str)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_parsed\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m df_csv\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#split_col = split(query['value'], ',')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#df = df.withColumn('NAME1', split_col.getItem(0))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#df = df.withColumn('NAME2', split_col.getItem(1))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# sum up the entries with the same key\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#popularity = pages.reduceByKey(lambda a, b: a+b)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StreamingQuery' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions  import split, from_csv\n",
    "col_schema = [\"line_no INTEGER\",\"host STRING\",\"time TIMESTAMP\",\"method STRING\",\"url STRING\",\"response INTEGER\",\"bytes INTEGER\"]\n",
    "schema_str = \",\".join(col_schema)\n",
    "\n",
    "df_csv = query.select(from_csv(query.value, schema_str).alias(\"value_parsed\"))\n",
    "df_csv.show()\n",
    "\n",
    "\n",
    "#split_col = split(query['value'], ',')\n",
    "#df = df.withColumn('NAME1', split_col.getItem(0))\n",
    "#df = df.withColumn('NAME2', split_col.getItem(1))\n",
    "\n",
    "#filter_pattern = re.compile(\".*GET,/.*\\.html\")\n",
    "#match_pattern = re.compile('GET,/.*\\.html')\n",
    "# select only lines with get Requests\n",
    "#filtered = input.filter(lambda s: filter_pattern.match(s)) \n",
    "\n",
    "# search for the match_pattern in each line and return the matching part as a key/value pair e.g. ('/ksc.html', 1) \n",
    "#pages = filtered.map(lambda s: (match_pattern.search(s).group()[4:], 1))\n",
    "# sum up the entries with the same key\n",
    "#popularity = pages.reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
