{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa App for nasa log view calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the necessary variables\n",
    "# The following links were used to determine the necessary packages to include:\n",
    "# - https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html and \n",
    "# - https://github.com/OneCricketeer/docker-stacks/blob/master/hadoop-spark/spark-notebooks/kafka-sql.ipynb  \n",
    "\n",
    "scala_version = '2.12'  \n",
    "spark_version = '3.5.3'\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "topic_name = 'kappa-topic'\n",
    "window_duration = '1 minute'\n",
    "sliding_duration = '1 minute'\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.9.0',\n",
    "    'org.mariadb.jdbc:mariadb-java-client:3.5.2'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "   .master(\"local\")\\\n",
    "   .appName(\"kafka-example\")\\\n",
    "   .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    "   .config(\"SQLConf.ADAPTIVE_EXECUTION_ENABLED.key\", \"false\")\\\n",
    "   .getOrCreate()\n",
    "#.config(\"spark.executor.extraClassPath\", \"/Users/sturm/Development/cloud_computing_big_data/lib/mariadb-java-client-3.5.2.jar\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaDf = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", *bootstrap_servers)\\\n",
    "  .option(\"subscribe\", topic_name)\\\n",
    "  .option(\"startingOffsets\", \"earliest\")\\\n",
    "  .load()\n",
    "print(kafkaDf.isStreaming)    # Returns True for DataFrames that have streaming sources\n",
    "kafkaDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure the value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit, split, from_unixtime\n",
    "#col_schema = [\"line_no INTEGER\",\"host STRING\",\"time TIMESTAMP\",\"method STRING\",\"url STRING\",\"response INTEGER\",\"bytes INTEGER\"]\n",
    "#schema_str = \",\".join(col_schema)\n",
    "structured_df = kafkaDf.select(\n",
    "    concat(col(\"topic\"), lit(':'), col(\"partition\").cast(\"string\")).alias(\"topic_partition\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"value\").cast(\"string\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"timestampType\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(0).alias(\"line_no\").cast(\"integer\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(1).alias(\"host\").cast(\"string\"),\n",
    "    from_unixtime(split(col(\"value\").cast(\"string\"),\",\").getItem(2)).alias(\"time\").cast(\"timestamp\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(3).alias(\"method\").cast(\"string\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(4).alias(\"url\").cast(\"VARCHAR(255)\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(5).alias(\"response\").cast(\"integer\"),\n",
    "    split(col(\"value\").cast(\"string\"),\",\").getItem(6).alias(\"bytes\").cast(\"integer\"),   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window,endswith,when, coalesce\n",
    "\n",
    "page_count_df = structured_df.filter(structured_df[\"method\"] == \"GET\")\\\n",
    "     .filter(structured_df[\"url\"].endswith(\".html\"))\\\n",
    "     .groupBy (window(\n",
    "        col(\"time\"),\n",
    "        window_duration,\n",
    "        sliding_duration\n",
    "    ),\n",
    "    col(\"url\")).count() \\\n",
    "     .withColumnRenamed('window.start', 'window_start') \\\n",
    "     .withColumnRenamed('window.end', 'window_end')\\\n",
    "     .withColumn(\"url\", coalesce(col(\"url\"), lit(\"unknown\")))\n",
    "page_count_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to MariaDB \n",
    "dbUrl = 'jdbc:mysql://localhost:3306/kappa-view?permitMysqlScheme' # Use of jdbc:mysql://HOST/DATABASE?permitMysqlScheme is MANDATORY\n",
    "dbOptions = {\"user\": \"root\", \"password\": \"example\", \"driver\": \"org.mariadb.jdbc.Driver\", \"truncate\": \"true\"}\n",
    "dbSchema = 'pagecount'\n",
    "\n",
    "def saveToDatabase(batchDataframe, batchId):\n",
    "    global dbUrl, dbSchema, dbOptions\n",
    "    print(f\"Writing batchID {batchId} to database @ {dbUrl}\")\n",
    "    batchDataframe.distinct().write.jdbc(dbUrl, dbSchema, \"overwrite\", dbOptions)\n",
    "\n",
    "\n",
    "db_insert_stream = page_count_df \\\n",
    "   .select(col('url'), col('count')) \\\n",
    "   .writeStream \\\n",
    "   .outputMode(\"complete\") \\\n",
    "   .foreachBatch(saveToDatabase) \\\n",
    "   .start()\n",
    "db_insert_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
